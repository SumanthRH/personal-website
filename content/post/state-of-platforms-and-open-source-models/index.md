---
title: The Current State of Platforms and Open Source Models 
subtitle: Some observations for how open source LLMs/SLMs are being used and predictions for the future 
date: 2024-09-22T05:37:15.089Z
summary: Some observations for how open source LLMs/SLMs are being used and predictions for the future
draft: false
featured: false
commentable: true
image:
  filename: featured
  focal_point: Smart
  preview_only: false
---
It's been almost 2 years since Meta released Llama-1, with incredible progress made in capabilities, applications and recipes for open source LLMs. I want to particularly lay down trends and some predictions for the future:

- Closed source adoption is way ahead: Aka OpenAI is still king. For all the hype you see on twitter from Anime pfps and elsewhere about the ever closing gap between open and closed sourced models, we should not lose sight of the truth that closed source model usage (API or chat usage) is still ahead: Recently, [OpenAI reported that ChatGPT has 200 million weekly active users](https://www.reuters.com/technology/artificial-intelligence/openai-says-chatgpts-weekly-users-have-grown-200-million-2024-08-29/), and there are some estimates that revenue from API usage is about [500 million](https://www.notoriousplg.ai/p/notorious-openais-revenue-breakdown). Open-source models are catching up on some measures in terms of quality, but in terms of adoption, closed source models are ahead. A LOT ahead. 
  - The price argument for open-source models, popular in 2023, is no longer valid in 2024, with a number of distilled models dropping, `gpt-4o-mini`, etc. This raises some interesting questions around how much control, ownership, maximum flexibility, etc - some of the (remaining) advantages of open-source models, will matter for most companies.
  - There's a clear funnel of important ideas and systems: You have closed source models and systems around working with closed source models (think: data management, eval frameworks etc) at the top, and then you have open source models and systems around inference with open source models, and lastly you have systems around customizing open source models. 
  - Open source model adoption, for plain chat use-cases, has definitely come far - this is primarily Meta AI -  all due to Meta's distribution ([Meta AI is recently reported to have 500 million monthly active users](https://techcrunch.com/2024/09/25/mark-zuckerberg-says-meta-ai-has-nearly-500-million-users/)). There is no "paid" category in this by the way, unlike with OpenAI/Anthropic, so the signals are stronger on the closed-source side of the line. 
- Llama as a platform (or _pretraining is unsexy again_): In Aug 2023, there were a number of players pre-training models, and it looked like that might as well remain the state of open-source: A number of research labs and Big Tech players releasing/ pre-training their own models either for it's own sake of building better models (Mistral) or tailored towards a particular application (CharacterAI). The story has changed significantly now, with consolidation around Llama as new "platform". For one datapoint - CharacterAI was recently [reverse acqui-hired](https://www.washingtonpost.com/technology/2024/08/02/google-character-ai-noam-shazeer/) by Google, and going-forward they've stated that their strategy is to actually shift to post-training models like Llama. Similarly, the capabilities of Llama, given the huge compute footprint and the talent pool of Meta, has surpassed other providers like Mistral. Two observations can be made:
    - _There will be more consolidation around Llama and more companies will switch to post-training or distilling Llama_
    - _Pretraining as a service will become less and less attractive._ There's definitely going to be lesser companies doing so as Meta itself invests heavily in pretraining + post-training Llama. This raises some interesting business questions - the stickyness of SAAS for pretraining is gonna decrease. I personally think pretraining large transformers will mostly disappear, and pretraining will be restricted to more research-driven exploration in the emerging architectures space, or in the category of smol models (GPU-poor).
- It is still early for companies finding value from LLMs: This is partly from my personal experience being involved with some customers at Anyscale, but mostly from keeping my ears perked up in tech. Most companies are still not quite sure about how best to use LLMs. Beyond the clear-cut use-cases for code generation, summarization, customer support chatbots, not much has moved the needle in terms of truly new use-cases in some time. The hype around AGI doesn't help either, and you'll probably see LLMs being used for tasks that they're really not a good fit for (_For a man with a hammer, everything starts looking like a nail_). In terms of priorities, reliability is I think probably the number one, and structured output generation is the second. 
  - There are interesting questions that emerge regarding open-source models from this. If a lot people are still in the exploratory stage of finding value i.e I have no idea what I'm doing with LLMs, then this entire category of companies will simply use the best closed source models available (GPT-4, etc). Only after the fog subsides and there's a clear path to value, companies will start thinking about open-source as an alternative for higher control and (sometimes) lower cost. 
  - Corollary: _Purely SAAS plays in infra for open-source models will not be attractive for a while given users are in the exploration stage with LLMs in general._
- Pytorch-native tools and libraries will become more and more popular for pre-training, fine-tuning and inference: If you've been to Pytorch Conference 2024, you'll see very clearly how Pytorch is investing heavily in building torch-native GenAI libraries - torchtitan, torchtune and torchchat. It's also very clear how far torch.compile (which, if you remember, has always been one of the, if not the, biggest focus areas for Pytorch 2.0) has come. The good thing about this investment is that there will be co-design of Pytorch feaetures for LLM use-cases, and in the same way co-design for good abstractions and performant implementations in torchtune, torchchat etc based on said features. Simplifying your tech stack towards native Pytorch will become more attractive as this happens. 
  - There's an interesting dynamic of _commoditization_ and tech stack convergence that happens with this - mainly in open source land. While there's still scope for pure performance optimizations, it's also clear why there's more focus on compound systems and the overall developer experience in the infra space. 
- Value capture is only recently starting to move up the chain: NVIDIA (monopoly hardware supplier) is still eating everyone's lunch, but we're seeing value capture move up the chain to cloud providers/ hyperscalers. Pure SAAS plays in this space (inference/fine-tuning/pretraining) will need to wait their turn. The exception to these simple buckets is data annotation providers (Scale AI, Surge AI, etc) who provide some form of human labour as a service.
